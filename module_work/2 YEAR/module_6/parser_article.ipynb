{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "from time import sleep, time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"articles_info.csv\"\n",
    "driver_path = \"/chromedriver\"\n",
    "base_dir = \"./data\"\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "start_time = time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_load_time(article_url, user_agent):\n",
    "    #будем ждать 3 секунды, иначе выводить exception и присваивать константное значение\n",
    "    try:\n",
    "        # меняем значение заголовка. По умолчанию указано, что это python-код\n",
    "        headers = {\n",
    "            \"User-Agent\": user_agent\n",
    "        }\n",
    "        # делаем запрос по url статьи article_url\n",
    "        response = requests.get(\n",
    "            article_url, headers=headers, stream=True, timeout=3.000\n",
    "        )\n",
    "        # получаем время загрузки страницы\n",
    "        load_time = response.elapsed.total_seconds()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        load_time = \">3\"\n",
    "    return load_time\n",
    "\n",
    "def write_to_file(output_list, filename, base_dir, firsr_run = False):\n",
    "    fieldnames = [\"id\", \"load_time\", \"rank\", \"points\", \"comments\", \"title\", \"url\"]\n",
    "\n",
    "    with open(Path(base_dir).joinpath(filename), \"w\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if firsr_run:\n",
    "            writer.writeheader()\n",
    "\n",
    "        for row in output_list:\n",
    "            writer.writerow(row)\n",
    "        csvfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_base(browser, page_number):\n",
    "    base_url = \"https://news.ycombinator.com/news?p={}\".format(page_number)\n",
    "    for connection_attempts in range(1,4): # совершаем 3 попытки подключения\n",
    "        try:\n",
    "            browser.get(base_url)\n",
    "            # ожидаем пока элемент table с id = 'hnmain' будет загружен на страницу\n",
    "            # затем функция вернет True иначе False\n",
    "            WebDriverWait(browser, 5).until(\n",
    "                EC.presence_of_element_located((By.ID, \"hnmain\"))\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error connecting to {}.\".format(base_url))\n",
    "            print(\"Attempt #{}.\".format(connection_attempts))\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDigitsFromStr(a):\n",
    "    num_list = []\n",
    "\n",
    "    num = ''\n",
    "    for char in a:\n",
    "        if char.isdigit():\n",
    "            num += str(char)\n",
    "\n",
    "    return num\n",
    "\n",
    "def parse_html(html, user_agent):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    output_list = []\n",
    "\n",
    "    # ищем в объекте soup object id, rank, score и title статьи\n",
    "    tr_blocks = soup.find_all(\"tr\", class_=\"athing\")\n",
    "    article = 0\n",
    "    for tr in tr_blocks:\n",
    "        article_id = tr.get(\"id\")  # id\n",
    "\n",
    "        article_url = ''\n",
    "        conteiner_url = tr.find_all('td', class_=\"title\")\n",
    "        if len(conteiner_url) == 2 and conteiner_url[1].find('a', href=True):\n",
    "            article_url = conteiner_url[1].find('a', href=True)['href']\n",
    "        else:\n",
    "            print('ХЕРОВО')\n",
    "            exit()\n",
    "\n",
    "        # article_url = tr.find_all(\"a\")[1][\"href\"]\n",
    "\n",
    "        # иногда статья располагается не на внешнем сайте, а на ycombinator, тогда article_url у нее не полный, а добавочный, с параметрами. Например item?id=200933. Для этих случаев будем добавлять урл до полного\n",
    "        if \"item?id=\" in article_url or \"from?site=\" in article_url:\n",
    "            article_url = f\"https://news.ycombinator.com/{article_url}\"\n",
    "        load_time = get_load_time(article_url, user_agent)\n",
    "        # иногда рейтинга может не быть, поэтому воспользуемся try\n",
    "\n",
    "        # значения по умолчанию\n",
    "        title = ''\n",
    "        score = ''\n",
    "        comments = 0\n",
    "        try:\n",
    "            if tr.find(class_=\"titlelink\"):\n",
    "                title = tr.find(class_=\"titlelink\").string\n",
    "\n",
    "            if soup.find(id=f\"score_{article_id}\"):\n",
    "                score = soup.find(id=f\"score_{article_id}\").string\n",
    "\n",
    "            if soup.find_all('a', href=f\"item?id={article_id}\"):\n",
    "                comm = soup.find_all('a', href=f\"item?id={article_id}\")\n",
    "                if len(comm) == 2:\n",
    "                    comments = getDigitsFromStr(comm[1].string)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            score = \"0 points\"\n",
    "\n",
    "        article_info = {\n",
    "            \"id\": str(article_id).strip(),\n",
    "            \"load_time\": str(load_time).strip(),\n",
    "            \"rank\": str(tr.span.string).strip(),\n",
    "            \"points\": str(score).strip(),\n",
    "            \"comments\": str(comments).strip(),\n",
    "            \"title\": str(title).strip(),\n",
    "            \"url\": str(article_url).strip(),\n",
    "        }\n",
    "        # добавляем информацию о статье в список\n",
    "        output_list.append(article_info)\n",
    "        article += 1\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-a530447b90c2>:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(executable_path='chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting page 0...\n",
      "getting page 1...\n",
      "HTTPSConnectionPool(host='naldc.nal.usda.gov', port=443): Read timed out. (read timeout=3.0)\n",
      "getting page 2...\n",
      "getting page 3...\n",
      "getting page 4...\n",
      "HTTPSConnectionPool(host='www.thetimes.co.uk', port=443): Read timed out. (read timeout=3.0)\n",
      "getting page 5...\n",
      "getting page 6...\n",
      "getting page 7...\n",
      "getting page 8...\n",
      "getting page 9...\n",
      "run time: 460.50700283050537 seconds\n"
     ]
    }
   ],
   "source": [
    "browser = webdriver.Chrome(executable_path='chromedriver.exe')\n",
    "\n",
    "first_run = True\n",
    "for page_number in range(10):\n",
    "    print(\"getting page \" + str(page_number) + \"...\")\n",
    "    if connect_to_base(browser, page_number):\n",
    "        sleep(5)\n",
    "        output_list = parse_html(browser.page_source, user_agent)\n",
    "\n",
    "        write_to_file(output_list, filename, base_dir, first_run)\n",
    "\n",
    "        if first_run:\n",
    "            first_run = False\n",
    "    else:\n",
    "        print(\"Error connecting to hacker news\")\n",
    "\n",
    "# завершаем работу драйвера\n",
    "browser.close()\n",
    "sleep(1)\n",
    "browser.quit()\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"run time: {} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
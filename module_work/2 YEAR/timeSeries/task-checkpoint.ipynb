{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Характеристики и параметры временных рядов. Практика\n",
    "1. Загрузите датасет train.csv.\n",
    "Задание будем выполнять для магазина с номером 25. Для этого сгруппируйте данные по дате и суммируйте количество всех продаж."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-1fc5b3933ce7>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'./data/train.csv'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstore_nbr\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m25\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"date\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'unit_sales'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'date'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_datetime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'date'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'%Y-%m-%d'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\study\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m                 )\n\u001B[1;32m--> 311\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    312\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\study\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    585\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 586\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    587\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    588\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\study\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    480\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    481\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 482\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    483\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    484\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\study\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    809\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    810\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 811\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    812\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    813\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\study\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1038\u001B[0m             )\n\u001B[0;32m   1039\u001B[0m         \u001B[1;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1040\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[call-arg]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1041\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1042\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\study\\venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m         \u001B[1;31m# open handles\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 51\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     52\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\study\\venv\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001B[0m in \u001B[0;36m_open_handles\u001B[1;34m(self, src, kwds)\u001B[0m\n\u001B[0;32m    220\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    221\u001B[0m         \"\"\"\n\u001B[1;32m--> 222\u001B[1;33m         self.handles = get_handle(\n\u001B[0m\u001B[0;32m    223\u001B[0m             \u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    224\u001B[0m             \u001B[1;34m\"r\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\study\\venv\\lib\\site-packages\\pandas\\io\\common.py\u001B[0m in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    700\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;34m\"b\"\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    701\u001B[0m             \u001B[1;31m# Encoding\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 702\u001B[1;33m             handle = open(\n\u001B[0m\u001B[0;32m    703\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    704\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './data/train.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/train.csv')\n",
    "df = df[df.store_nbr == 25].copy()\n",
    "df = df.groupby([\"date\"])['unit_sales'].sum().reset_index()\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x = 'date', y = 'unit_sales',  figsize=(15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как данные содержат пропуски, создадим врменной промежуток и пропуски заполним нулем!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeDay = pd.date_range(df['date'].min(), end=df['date'].max())\n",
    "union = pd.DataFrame()\n",
    "union[\"date\"] = timeDay\n",
    "union = pd.merge(left=union, right=df[[\"date\", 'unit_sales']], how=\"left\",on=[\"date\"])\n",
    "union[\"unit_sales\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Cезонная декомпозиция."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union = union.set_index(pd.DatetimeIndex(union['date']))\n",
    "union.drop(['date'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = seasonal_decompose(union, model='additive')\n",
    "trend_part = decomposition.trend\n",
    "seasonal_part = decomposition.seasonal\n",
    "residual_part = decomposition.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_part.plot(x = 'date', y = 'unit_sales',  figsize=(15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_part = trend_part.dropna()\n",
    "print(\"Размер трендовой составляющей \" + str(trend_part.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_part.plot(x = 'date', y = 'unit_sales',  figsize=(15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_part = seasonal_part.dropna()\n",
    "print(\"Размер сезональной составляющей \" + str(seasonal_part.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_part.plot(x = 'date', y = 'unit_sales',  figsize=(15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_part = trend_part.dropna()\n",
    "print(\"Размер случаной составляющей \" + str(trend_part.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тест  Дики-Фуллера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adfulleftDef(df):\n",
    "    test = adfuller(df[\"unit_sales\"])\n",
    "    print('adf: ', test[0])\n",
    "    print('p-value: ', test[1])\n",
    "    print('Critical values: ', test[4])\n",
    "    if test[0] > test[4]['5%']:  # проверка, больше ли критического полученное значение для нашего ряда\n",
    "        print('ряд не стационарен')\n",
    "    else:\n",
    "        print('ряд стационарен')\n",
    "adfulleftDef(union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобъем временной ряд на три"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit()\n",
    "train_test_groups = tscv.split(union[\"unit_sales\"] )\n",
    "for train_index, test_index in train_test_groups:\n",
    "    print(\"TRAIN size:\", len(train_index), \"TEST size:\", len(test_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Oконные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Upper(value):\n",
    "   return value.mean() + 3 * value.std()\n",
    "def Lower(value):\n",
    "   return value.mean() - 3 * value.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте скользящее среднее c окном 5\n",
    "Постройте скользящее квадратическое отклонение c окном 5. \n",
    "остройте так называемые линии Боллинджера с окном в 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union['sales_mean'] = union[\"unit_sales\"].rolling(window=5).mean()\n",
    "union['sales_std'] = union[\"unit_sales\"].rolling(window=5).std()\n",
    "\n",
    "union['upper']  = union[\"unit_sales\"].rolling(window=30).apply(Upper, raw=False)\n",
    "union['lower']  = union[\"unit_sales\"].rolling(window=30).apply(Lower, raw=False)\n",
    "\n",
    "union['date'] = union.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## визуализируем\n",
    "f, ax = plt.subplots(1, 1, figsize=(18, 8))\n",
    "ax.plot(union[\"date\"], union[\"unit_sales\"], c='black') ## исходные данные\n",
    "ax.plot(union[\"date\"], union[\"sales_mean\"], c='slateblue') ## среднее данные\n",
    "ax.plot(union[\"date\"], union[\"sales_std\"], c='salmon') ##  квадратическое отклонение \n",
    "\n",
    "ax.plot(union[\"date\"], union[\"lower\"], c='lime') ## нижняя граница Боллинджера \n",
    "ax.plot(union[\"date\"], union[\"upper\"], c='darkgreen') ## вверхняя граница Боллинджера \n",
    "\n",
    "\n",
    "ax.legend(['unit_sales', 'sales_mean', 'sales_std', \"lower\", 'upper'])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Постройте оконное среднее с окном в 10 точек.\n",
    "N = 10\n",
    "union['sales_mean '  + str(N)] = union[\"unit_sales\"].rolling(window=10).mean()\n",
    "N = 7\n",
    "union['sales_ewn '  + str(N)] = union[\"unit_sales\"].ewm(min_periods=N, span=N).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(18, 8))\n",
    "ax.plot(union[\"date\"], union[\"sales_mean 10\"], c='black') ## среднее с окном в 10 точек\n",
    "ax.plot(union[\"date\"], union[\"sales_ewn 7\"], c='slateblue') ## экспоненциально среднее с окном в 7 точек\n",
    "\n",
    "ax.legend(['unit_sales', 'sales_mean', 'sales_std', \"lower\", 'upper'])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите свою скользящую функцию, которая будет являться средним арифметическим между максимальным и минимальным значением unit_sales, используя окно 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MeanMaxMin(value):\n",
    "    return (value.max() + value.min()) / 2\n",
    "union['sales_func'] = union[\"unit_sales\"].rolling(window=10).apply(MeanMaxMin, raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Постройте скользящее среднее с окном 50 и экспоненциальное скользящее среднее с окном 10 (span=10).\n",
    "#Укажите индексы датафрейма, на которых одно скользящее пересекается с другим\n",
    "union['sales_mean_50'] = union[\"unit_sales\"].rolling(window=50).mean()\n",
    "union['sales_ewn_10'] = union[\"unit_sales\"].ewm(min_periods=10, span=10).mean()\n",
    "\n",
    "union['diff'] = union['sales_mean_50'] - union['sales_ewn_10']\n",
    "union['diff'] = np.sign(union['diff']).diff()\n",
    "union[union['diff'] != 0]['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получите лаговые факторы с 7-го по 10-й предыдущих дня (лаг7, лаг8, лаг9, лаг10) и отобразите на графике\n",
    "union = union[['date', 'unit_sales']]\n",
    "\n",
    "for i in range(7,11):\n",
    "    union['sale_{}'.format(i)] = union.unit_sales.shift(i)\n",
    "    \n",
    "## визуализируем\n",
    "f, ax = plt.subplots(1, 1, figsize=(18, 8))\n",
    "ax.plot(union.date, union.unit_sales) ## исходные данные\n",
    "ax.plot(union.date, union['sale_' + str(7)],  c='g') \n",
    "ax.plot(union.date, union['sale_' + str(8)],  c='r') \n",
    "ax.plot(union.date, union['sale_' + str(9)],  c='b') \n",
    "ax.plot(union.date, union['sale_' + str(10)],  c='y') \n",
    "ax.legend(['unit_sales', 'sale_7', 'sale_8', 'sale_9', 'sale_10'])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(18, 8))\n",
    "ax.plot(union.date[:100], union.unit_sales[:100]) ## исходные данные\n",
    "ax.plot(union.date[:100], union['sale_' + str(7)][:100],  c='g') \n",
    "ax.plot(union.date[:100], union['sale_' + str(8)][:100],  c='r') \n",
    "ax.plot(union.date[:100], union['sale_' + str(9)][:100],  c='b') \n",
    "ax.plot(union.date[:100],  union['sale_' + str(10)][:100],  c='y') \n",
    "ax.legend(['unit_sales','sale_7', 'sale_8', 'sale_9', 'sale_10'])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA/SARIMA. Практика\n",
    "\n",
    "#### Импортируем метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "top1 = train[train.item_nbr == 103501]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1['date'] = pd.to_datetime(top1['date'])\n",
    "top1['year'] = top1['date'].dt.year\n",
    "unit_sales_by_date = top1.groupby('date').sum()['unit_sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тест Адфуллера для проверки стационарных временных рядов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "X = unit_sales_by_date.values\n",
    "result = adfuller(X)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "if result[0] < result[4][\"5%\"]:\n",
    "    print (\"Reject Ho - Time Series is Stationary\")\n",
    "else:\n",
    "    print (\"Failed to Reject Ho - Time Series is Non-Stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель скользящего среднего"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_forecast(series, window_size):\n",
    "    forecast = []\n",
    "\n",
    "    for time in range(len(series) - window_size):\n",
    "        forecast.append(series[time:time + window_size].mean())\n",
    "    return np.array(forecast)\n",
    "\n",
    "moving_average_days = 6\n",
    "shown_train_size = moving_average_days * 3\n",
    "\n",
    "moving_avg = moving_average_forecast(unit_sales_by_date,moving_average_days)\n",
    "moving_avg = pd.Series(moving_avg, index = unit_sales_by_date[moving_average_days:].index)\n",
    "\n",
    "print(moving_avg[-moving_average_days:].shape,unit_sales_by_date[-moving_average_days:].shape)\n",
    "\n",
    "print( \"mean_squared_error\", mean_squared_error(\n",
    "    unit_sales_by_date.values[-moving_average_days:],\n",
    "    moving_avg[-moving_average_days:]\n",
    "))\n",
    "print(\"mean_absolute_error\", mean_absolute_error(\n",
    "    unit_sales_by_date.values[-moving_average_days:],\n",
    "    moving_avg[-moving_average_days:]))\n",
    "\n",
    "print(\"mean_absolute_percentage_error\", mean_absolute_percentage_error(\n",
    "    unit_sales_by_date.values[-moving_average_days:],\n",
    "    moving_avg[-moving_average_days:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.plot(unit_sales_by_date[-shown_train_size:], label=\"Actual\")\n",
    "plt.plot(moving_avg[-moving_average_days:], label=\"Moving average prediction\")\n",
    "plt.ylabel(\"Total Unit Sold\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.title(\"Moving Average Forecast\")\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель линейной регресии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_size = 6\n",
    "df = pd.DataFrame()\n",
    "df[\"Original Values\"]  = unit_sales_by_date\n",
    "df[\"shift7\"] = df[\"Original Values\"].shift(predict_size)\n",
    "df[\"shift8\"] = df[\"shift7\"].shift()\n",
    "df[\"shift9\"] = df[\"shift8\"].shift()\n",
    "df[\"shift10\"] = df[\"shift9\"].shift()\n",
    "df[\"shift11\"] = df[\"shift10\"].shift()\n",
    "df[\"shift12\"] = df[\"shift11\"].shift()\n",
    "df[\"shift13\"] = df[\"shift12\"].shift()\n",
    "df.dropna(inplace=True)\n",
    "#\n",
    "x_train, y_train = df[:-predict_size].drop([\"Original Values\"], axis =1), df[:-predict_size][\"Original Values\"]\n",
    "x_test, y_test  = df[-predict_size:].drop([\"Original Values\"], axis =1), df[-predict_size:][\"Original Values\"]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(x_train, y_train)\n",
    "# print(reg.coef_)\n",
    "# print(reg.intercept_)\n",
    "\n",
    "ar_predictions = pd.Series(reg.predict(x_test), index=x_test.index)\n",
    "\n",
    "print(\"mean_squared_error\",mean_squared_error(y_test, ar_predictions))\n",
    "print(\"mean_absolute_error\",mean_absolute_error(y_test, ar_predictions))\n",
    "print(\"mean_absolute_percentage_error\", mean_absolute_percentage_error(y_test, ar_predictions))\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(ar_predictions ,label = \"Predictions\")\n",
    "plt.plot(pd.concat([y_train, y_test], axis = 0)[-shown_train_size:], label = \"Original\" )\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Sale Count\")\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение модели ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(unit_sales_by_date.values.squeeze(), lags=10, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(unit_sales_by_date, lags=10, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, чтобы построить ARIMA модель нам нужно знать ее порядок, состоящий из 3-х параметров:\n",
    "\n",
    "p — порядок компоненты AR\n",
    "\n",
    "d — порядок интегрированного ряда\n",
    "\n",
    "q — порядок компонетны MA\n",
    "\n",
    "d мы уже знаем - это 0 (ряд стационарный! )\n",
    "\n",
    "осталось определить p и q. Для их определения нам надо изучить авторкорреляционную(ACF) и частично автокорреляционную(PACF) функции для ряда первых разностей.\n",
    "\n",
    "ACF поможет нам определить q, т. к. по ее коррелограмме можно определить количество автокорреляционных коэффициентов сильно отличных от 0 в модели MA Таким образом q = 8\n",
    "\n",
    "\n",
    "PACF поможет нам определить p, т. к. по ее коррелограмме можно определить максимальный номер коэффициента сильно отличный от 0 в модели AR. p = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "model = ARIMA(y_train.values.reshape(-1), order=(8,0,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_predict\n",
    "\n",
    "train_size = len(y_train)\n",
    "test_size = predict_size\n",
    "arima_predictions = model.fit().predict(start=train_size,end=train_size+test_size -1,  dynamic=False)\n",
    "\n",
    "plt.plot(pd.Series(arima_predictions, index=y_test.index) ,label = \"Predictions\")\n",
    "plt.plot(pd.concat([y_train, y_test], axis = 0)[-shown_train_size:], label = \"Original\" )\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean_squared_error\",mean_squared_error(y_test, arima_predictions))\n",
    "print(\"mean_absolute_error\",mean_absolute_error(y_test, arima_predictions))\n",
    "print(\"mean_absolute_percentage_error\", mean_absolute_percentage_error(y_test, arima_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поробуем модель (6,0,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(y_train.values.reshape(-1), order=(6,0,6))\n",
    "\n",
    "train_size = len(y_train)\n",
    "test_size = predict_size\n",
    "arima_predictions = model.fit().predict(start=train_size,end=train_size+test_size -1,  dynamic=False)\n",
    "\n",
    "plt.plot(pd.Series(arima_predictions, index=y_test.index) ,label = \"Predictions\")\n",
    "plt.plot(pd.concat([y_train, y_test], axis = 0)[-shown_train_size:], label = \"Original\" )\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean_squared_error\",mean_squared_error(y_test, arima_predictions))\n",
    "print(\"mean_absolute_error\",mean_absolute_error(y_test, arima_predictions))\n",
    "print(\"mean_absolute_percentage_error\", mean_absolute_percentage_error(y_test, arima_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель (6,0,6) лучше описывает данные чем модель (8,0,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод:\n",
    "\n",
    "    - смоделированы методы описания временного ряда (скользящего среднего, линейной регресии, ARIMA)\n",
    "    \n",
    "    - по результатам скользящее среднее - 0.17 (MAPE), линейная регрессия - 0.17, arima - 0.12\n",
    "    \n",
    " Модель arima существенно лучше предсказывает временной ряд!    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPProphet. Практика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 70)\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode()\n",
    "\n",
    "from fbprophet import Prophet\n",
    "\n",
    "%time df_transactions = pd.read_csv('./data/transactions.csv')\n",
    "%time df_holidays_events = pd.read_csv('./data/holidays_events.csv')\n",
    "\n",
    "print('Data and libraries are loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = df_transactions.groupby('date')['transactions'].sum()\n",
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot([go.Scatter(\n",
    "    x=transactions.index,\n",
    "    y=transactions\n",
    ")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошо заметно влияние сезонности и праздников на общий объем транзакций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Предсказание пророком (prophet) на год вперёд (periods=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.DataFrame(transactions).reset_index()\n",
    "transactions.columns = ['ds', 'y']\n",
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet()\n",
    "m.fit(transactions)\n",
    "future = m.make_future_dataframe(periods=365)\n",
    "forecast = m.predict(future)\n",
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot([\n",
    "    go.Scatter(x=transactions['ds'], y=transactions['y'], name='y'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['yhat'], name='yhat'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['yhat_upper'], fill='tonexty', mode='none', name='upper'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['yhat_lower'], fill='tonexty', mode='none', name='lower'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['trend'], name='Trend')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычислим среднеквадратичную ошибку.\n",
    "print('RMSE: %f' % np.sqrt(np.mean((forecast.loc[:1682, 'yhat']-transactions['y'])**2)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поэкспериментируйте с коэффициентом changepoint_prior_scale\n",
    "for item in [0.25,0.5,1.0,2.5,4.0]:\n",
    "    m = Prophet(changepoint_prior_scale=item)\n",
    "    m.fit(transactions)\n",
    "    future = m.make_future_dataframe(periods=365)\n",
    "    forecast = m.predict(future)\n",
    "    print('RMSE: %f' % np.sqrt(np.mean((forecast.loc[:1682, 'yhat']-transactions['y'])**2)),\n",
    "         'changepoint_prior_scale: %f' % item )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet(changepoint_prior_scale=8.0)\n",
    "m.fit(transactions)\n",
    "future = m.make_future_dataframe(periods=365)\n",
    "forecast = m.predict(future)\n",
    "print('RMSE: %f' % np.sqrt(np.mean((forecast.loc[:1682, 'yhat']-transactions['y'])**2)) )\n",
    "\n",
    "py.iplot([\n",
    "    go.Scatter(x=transactions['ds'], y=transactions['y'], name='y'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['yhat'], name='yhat'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['yhat_upper'], fill='tonexty', mode='none', name='upper'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['yhat_lower'], fill='tonexty', mode='none', name='lower'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['trend'], name='Trend')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличение параметра changepoint_prior_scale делать тренд более изменичивым! \n",
    "\n",
    "##### Добавим в модель месячную сезонность (name='monthly', period=30.5, fourier_order=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet(changepoint_prior_scale=8.0)\n",
    "m.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "m.fit(transactions)\n",
    "future = m.make_future_dataframe(periods=365)\n",
    "forecast = m.predict(future)\n",
    "print('RMSE: %f' % np.sqrt(np.mean((forecast.loc[:1682, 'yhat']-transactions['y'])**2)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot([\n",
    "    go.Scatter(x=transactions['ds'], y=transactions['y'], name='y'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['yhat'], name='yhat'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['yhat_upper'], fill='tonexty', mode='none', name='upper'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['yhat_lower'], fill='tonexty', mode='none', name='lower'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['trend'], name='Trend')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Улучшите модель добавлением данных о праздниках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holidays_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = df_holidays_events[df_holidays_events['transferred'] == False][['description', 'date']]\n",
    "holidays.columns = ['holiday', 'ds']\n",
    "holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet(changepoint_prior_scale=8.0, holidays=holidays)\n",
    "m.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "m.fit(transactions)\n",
    "future = m.make_future_dataframe(periods=365)\n",
    "forecast = m.predict(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE: %f' % np.sqrt(np.mean((forecast.loc[:1682, 'yhat']-transactions['y'])**2)) )\n",
    "py.iplot([\n",
    "    go.Scatter(x=transactions['ds'], y=transactions['y'], name='y'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['yhat'], name='yhat'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['yhat_upper'], fill='tonexty', mode='none', name='upper'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['yhat_lower'], fill='tonexty', mode='none', name='lower'),\n",
    "    go.Scatter(x=forecast['ds'], y=forecast['trend'], name='Trend')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод:\n",
    "\n",
    "    - модель FPProphet позволяет прогнозировать данные рядов\n",
    "    \n",
    "    - у модели FPProphet есть параметры параметры которые позволяют уменьшаьб ошибку , сезонность , праздникик и  тп  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прогнозирование временных рядов с использованием XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance, plot_tree\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjme = pd.read_csv('./data/PJME_hourly.csv', index_col=[0], parse_dates=[0])\n",
    "pjme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_pal = [\"#F8766D\", \"#D39200\", \"#93AA00\", \"#00BA38\", \"#00C19F\", \"#00B9E3\", \"#619CFF\", \"#DB72FB\"]\n",
    "_ = pjme.plot(style='.', figsize=(25,10), color=color_pal[0], title='PJM East')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отрежем данные после 2015 года, чтобы использовать их в качестве набора для проверки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '01-Jan-2015'\n",
    "pjme_train = pjme.loc[pjme.index <= split_date].copy()\n",
    "pjme_test = pjme.loc[pjme.index > split_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjme_test \\\n",
    "    .rename(columns={'PJME_MW': 'TEST SET'}) \\\n",
    "    .join(pjme_train.rename(columns={'PJME_MW': 'TRAINING SET'}), how='outer') \\\n",
    "    .plot(figsize=(15,5), title='PJM East', style='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим признаки\n",
    "def create_features(df, label=None):\n",
    "    \"\"\"\n",
    "    создаем признаки из datetime индекса\n",
    "    \"\"\"\n",
    "    df['date'] = df.index\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['dayofmonth'] = df['date'].dt.day\n",
    "    df['weekofyear'] = df['date'].dt.weekofyear\n",
    "    \n",
    "    X = df[['hour','dayofweek','quarter','month','year',\n",
    "           'dayofyear','dayofmonth','weekofyear']]\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = create_features(pjme_train, label='PJME_MW')\n",
    "X_test, y_test = create_features(pjme_test, label='PJME_MW')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = xgb.XGBRegressor(n_estimators=1000)\n",
    "reg.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        early_stopping_rounds=150,\n",
    "       verbose=False) # Измените verbose на True, если хотите увидеть процесс обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Важность признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_importance(reg, height=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание на Test Set\n",
    "pjme_test['MW_Prediction'] = reg.predict(X_test)\n",
    "pjme_all = pd.concat([pjme_test, pjme_train], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pjme_all[['PJME_MW','MW_Prediction']].plot(figsize=(25, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ошибки на Test Set\n",
    "mean_squared_error(y_true=pjme_test['PJME_MW'],\n",
    "                   y_pred=pjme_test['MW_Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_true=pjme_test['PJME_MW'],\n",
    "                   y_pred=pjme_test['MW_Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_true=pjme_test['PJME_MW'],\n",
    "                   y_pred=pjme_test['MW_Prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на худшие и лучшие прогнозируемые дни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjme_test['error'] = pjme_test['PJME_MW'] - pjme_test['MW_Prediction']\n",
    "pjme_test['abs_error'] = pjme_test['error'].apply(np.abs)\n",
    "error_by_day = pjme_test.groupby(['year','month','dayofmonth']) \\\n",
    "    .mean()[['PJME_MW','MW_Prediction','error','abs_error']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Худшие абсолютные прогнозируемые дни\n",
    "error_by_day.sort_values('abs_error', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лучшие прогнозируемые дни\n",
    "error_by_day.sort_values('abs_error', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравним с CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbr = CatBoostRegressor(n_estimators=1000)\n",
    "cbr.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        early_stopping_rounds=50,\n",
    "       verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjme_test['MW_Prediction_catboost'] = cbr.predict(X_test)\n",
    "pjme_all = pd.concat([pjme_test, pjme_train], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_true=pjme_test['PJME_MW'],\n",
    "                   y_pred=pjme_test['MW_Prediction_catboost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_true=pjme_test['PJME_MW'],\n",
    "                   y_pred=pjme_test['MW_Prediction_catboost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_true=pjme_test['PJME_MW'],\n",
    "                   y_pred=pjme_test['MW_Prediction_catboost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost превосходит модель CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введенем новые признаки,  праздники \n",
    "Пересоздадим датареймы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "pjme = pd.read_csv('./data/PJME_hourly.csv', index_col=[0], parse_dates=[0])\n",
    "\n",
    "split_date = '01-Jan-2015'\n",
    "pjme_train = pjme.loc[pjme.index <= split_date].copy()\n",
    "pjme_test = pjme.loc[pjme.index > split_date].copy()\n",
    "\n",
    "# Создадим признаки\n",
    "def create_features(df, label=None):\n",
    "    us_holidays = holidays.country_holidays('US')\n",
    "    \"\"\"\n",
    "    создаем признаки из datetime индекса\n",
    "    \"\"\"\n",
    "    df['date'] = df.index\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['dayofmonth'] = df['date'].dt.day\n",
    "    df['weekofyear'] = df['date'].dt.weekofyear\n",
    "\n",
    "    df['holiday'] = df.apply(lambda row: 1 if row[\"date\"] in us_holidays else 0, axis=1)\n",
    "    X = df[['hour', 'dayofweek', 'quarter', 'month', 'year',\n",
    "            'dayofyear', 'dayofmonth', 'weekofyear', \"holiday\"]]\n",
    "\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X\n",
    "\n",
    "X_train, y_train = create_features(pjme_train, label='PJME_MW')\n",
    "X_test, y_test = create_features(pjme_test, label='PJME_MW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = xgb.XGBRegressor(n_estimators=1000)\n",
    "reg.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        early_stopping_rounds=50,\n",
    "       verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_importance(reg, height=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjme_test['MW_Prediction'] = reg.predict(X_test)\n",
    "pjme_all = pd.concat([pjme_test, pjme_train], sort=False)\n",
    "_ = pjme_all[['PJME_MW','MW_Prediction']].plot(figsize=(25, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_true=pjme_test['PJME_MW'],\n",
    "                   y_pred=pjme_test['MW_Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjme_test['error'] = pjme_test['PJME_MW'] - pjme_test['MW_Prediction']\n",
    "pjme_test['abs_error'] = pjme_test['error'].apply(np.abs)\n",
    "error_by_day = pjme_test.groupby(['year','month','dayofmonth']) \\\n",
    "    .mean()[['PJME_MW','MW_Prediction','error','abs_error']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Худшие абсолютные прогнозируемые дни\n",
    "error_by_day.sort_values('abs_error', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лучшие прогнозируемые дни\n",
    "error_by_day.sort_values('abs_error', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CatBoostRegressor\n",
    "cbr = CatBoostRegressor(n_estimators=1000)\n",
    "cbr.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        early_stopping_rounds=50,\n",
    "       verbose=False)\n",
    "pjme_test['MW_Prediction_catboost'] = cbr.predict(X_test)\n",
    "pjme_all = pd.concat([pjme_test, pjme_train], sort=False)\n",
    "mean_absolute_percentage_error(y_true=pjme_test['PJME_MW'],\n",
    "                   y_pred=pjme_test['MW_Prediction_catboost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод введение дополнительной переменной праздники не улучшило результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введенем новые признаки,  лаги "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjme = pd.read_csv('./data/PJME_hourly.csv', index_col=[0], parse_dates=[0])\n",
    "\n",
    "split_date = '01-Jan-2015'\n",
    "pjme_train = pjme.loc[pjme.index <= split_date].copy()\n",
    "pjme_test = pjme.loc[pjme.index > split_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, label=None):\n",
    "    us_holidays = holidays.country_holidays('US')\n",
    "    \"\"\"\n",
    "    создаем признаки из datetime индекса\n",
    "    \"\"\"\n",
    "    df['date'] = df.index\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['dayofmonth'] = df['date'].dt.day\n",
    "    df['weekofyear'] = df['date'].dt.weekofyear\n",
    "\n",
    "    df['holiday'] = df.apply(lambda row: 1 if row[\"date\"] in us_holidays else 0, axis=1)\n",
    "\n",
    "    predictDA = 24\n",
    "    predict2DA = 48\n",
    "    df[\"ShiftOne\"] = df[label].shift(predictDA)\n",
    "    df[\"ShiftTwo\"] = df[label].shift(predict2DA)\n",
    "    X = df[['hour', 'dayofweek', 'quarter', 'month', 'year',\n",
    "            'dayofyear', 'dayofmonth', 'weekofyear', \"holiday\", \"ShiftOne\", \"ShiftTwo\"]]\n",
    "\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X\n",
    "X_train, y_train = create_features(pjme_train, label='PJME_MW')\n",
    "X_test, y_test = create_features(pjme_test, label='PJME_MW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = xgb.XGBRegressor(n_estimators=1000)\n",
    "reg.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        early_stopping_rounds=50,\n",
    "       verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_importance(reg, height=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjme_test['MW_Prediction'] = reg.predict(X_test)\n",
    "pjme_all = pd.concat([pjme_test, pjme_train], sort=False)\n",
    "_ = pjme_all[['PJME_MW','MW_Prediction']].plot(figsize=(25, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_true=pjme_test['PJME_MW'],\n",
    "                   y_pred=pjme_test['MW_Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjme_test['error'] = pjme_test['PJME_MW'] - pjme_test['MW_Prediction']\n",
    "pjme_test['abs_error'] = pjme_test['error'].apply(np.abs)\n",
    "error_by_day = pjme_test.groupby(['year','month','dayofmonth']) \\\n",
    "    .mean()[['PJME_MW','MW_Prediction','error','abs_error']]\n",
    "# Худшие абсолютные прогнозируемые дни\n",
    "error_by_day.sort_values('abs_error', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CatBoostRegressor\n",
    "cbr = CatBoostRegressor(n_estimators=1000)\n",
    "cbr.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        early_stopping_rounds=50,\n",
    "       verbose=False)\n",
    "pjme_test['MW_Prediction_catboost'] = cbr.predict(X_test)\n",
    "pjme_all = pd.concat([pjme_test, pjme_train], sort=False)\n",
    "mean_absolute_percentage_error(y_true=pjme_test['PJME_MW'],\n",
    "                   y_pred=pjme_test['MW_Prediction_catboost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: Ввдение лагов существенно на существенно величило прогноз.\n",
    "\n",
    "Таким образом было рассмотрено прогнозирование временных рядов c помощью различных алгоритмов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}